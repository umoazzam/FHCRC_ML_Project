# - `median`: in which you'll store the median of the vector
summary_info <- list(
length = numbers_len,
mean = numbers_mean,
median = numbers_median
)
# Now, write a function called `summarize_vector` that takes in a vector of
# numbers, and returns a list of summary information about that vector
# (including the mean, median, and length)
summarize_vector <- function(num_vec) {
return_list <- list(
num_length = length(num_vec),
num_mean = mean(num_vec),
num_median = median(num_vec)
)
}
# Create a list `summary_1_to_100` by passing a vector of the values one
# through one hundred to your `summarize_vector` function
summary_1_to_100 <- summarize_vector(1:100)
# Create a vector `students` holding 1,000 values representing students
# They should have the values "Student 1", "Student 2",..., "Student 1000"
students <- 1:1000
# Create a vector `math_grades` that holds 1000 random values in it that
# represent grades in a math course
# These values should be normally distributed with a mean of 88 and a
# standard deviation of 10
# Hint: Lookup `rnorm()`
math_grades <- rnorm(1000, 88, 10)
# In the `math_grades` vector, replace any values that are above 100 with
# the number 100
# Hint: Vector filtering
math_grades[math_grades > 100] <- 100
# Create a vector `spanish_grades` that holds 1000 random values in it that
# represent grades in a spanish course
# These values should be normally distributed with a mean of 85 and a
# standard deviation of 12
# Hint: Lookup `rnorm()`
spanish_grades <- rnorm(1000, 85, 12)
# In the `spanish_grades` vector, replace any values that are above 100 with
# the number 100
# Hint: More vector filtering
spanish_grades[spanish_grades > 100] <- 100
# Create a data frame variable named `grades` by combining
# the vectors `students`, `math_grades`, and `spanish_grades`
# Make sure to properly handle strings
grades <- data.frame(
students,
math_grades,
spanish_grades
)
# Create a variable `num_students` that counts the
# number of rows in your dataframe `grades`
num_students <- nrow(grades)
# Create a variable `num_courses` that counts the number of courses stored
# in the columns of your `grades` data frame
num_courses <- ncol(grades[, 2:3])
# Add a new column `grade_diff` to your data frame, which is equal to
# `grades$math_grades` minus `grades$spanish_grades`
grades$grade_diff <- grades$math_grades - grades$spanish_grades
# Add another column `better_at_math` as a boolean (TRUE/FALSE) variable that
# indicates that a student got a better grade in math
grades$better_at_math <- grades$grade_diff > 0
# Create a variable `num_better_at_math` that is the number
# (i.e., one numeric value) of students better at math
num_better_at_math <- length(
grades$better_at_math[grades$better_at_math == TRUE]
)
data()
# Create a vector of the number of points the Seahawks scored in the first 4 games
# of the season (google "Seahawks" for the scores!)
sh_points <- c(17, 23, 21, 28)
# Create a vector of the number of points the Seahwaks have allowed to be scored
# against them in each of the first 4 games of the season
opp_points <- c(9, 28, 20, 26)
# Combine your two vectors into a dataframe called `games`
game <- data.frame(sh_points, opp_points)
# Create a new column "diff" that is the difference in points between the teams
# Hint: recall the syntax for assigning new elements (which in this case will be
# a vector) to a list!
game$diff <- game$sh_points - game$opp_points
# Create a new column "won" which is TRUE if the Seahawks won the game
game$won <- game$diff > 0
# Create a vector of the opponent names corresponding to the games played
opp_names <- c("Eagles", "Packers", "Bengals", "Steelers")
# Assign your dataframe rownames of their opponents
row.names(game) <- opp_names
View(game)
# View your data frame to see how it has changed!
VIew(game)
data(iris)
View(iris)
#Get all rows of Species ???versicolor??? in a new data frame. Call this data frame: ???iris.vers???
???iris.vers??? <- data.frame(iris[,"versicolor"])
#Get all rows of Species ???versicolor??? in a new data frame. Call this data frame: ???iris.vers???
iris_vers <- data.frame(iris[,"versicolor"])
source('~/info201/class-exercise-uamoazzam/chapter-11-exercises/exercise-4/exercise.R', echo=TRUE)
install.packages("nycflights13")
source('~/info201/class-exercise-uamoazzam/chapter-11-exercises/exercise-4/exercise.R', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE)
#include for kable
library("knitr")
source("analysis.R")
#include for kable
library("knitr")
source("analysis.R")
kable(idv_sumarry_df)
kable(idv_summary_df)
install.packages("ggplot2")
install.packages("tidyverse")
library(ggplot2)
ggplot(mpg, aes(x = displ, y = hwy)) +
geom_point(())
geom_point(
ggplot(mpg, aes(x = displ, y = hwy)) +
geom_point()
ggplot(mpg, aes(x = displ, y = hwy)) +
geom_point()
ggplot(mpg, aes(x = displ, y = hwy)) +
geom_point()
ggplot(mpg, aes(displ, cty, colour = class)) +
geom_point()
install.packages(c("ggthemes", "gridExtra", "Hmisc"))
# First, let's review the mtcars datset
str(mtcars)
View(mtcars)
# Simple scatter plot
g <- ggplot(data = mtcars, mapping = aes(x = mpg, y = hp))
g + geom_point() # <- Notice that this function inherits the data and aesthetic of g so no arguments are required
?geom_point # check out the documentation
# We can, however, change the data or add other aesthetics that will only be applied to the scatterplot
g + geom_point(aes(alpha = 0.7), # useful to turn this down if you have a lot of points in the same area
color = "red",
shape = 3, # I've almost never used shape and advise against it, in general
size = 10, # relative size
stroke = 3) # boldness
# With ggplot, something to remember is that you can continue to add more elements, including the same elements
g + geom_point(aes(alpha = 0.7), # useful to turn this down if you have a lot of points in the same area
color = "red",
shape = 3, # I've almost never used shape and advise against it, in general
size = 10, # relative size
stroke = 1) + # boldness
geom_point(aes(alpha = 0.7), # useful to turn this down if you have a lot of points in the same area
color = "black",
shape = 1, # I've almost never used shape and advise against it, in general
size = 10, # relative size
stroke = 2)
# Seems like we'll want some labels:
?labs # check out documentation
g + geom_point() +
labs(title = "Title of the plot",
subtitle = "Subtitle",
caption = "An insightful caption",
x = "x-axis title",
y = "y-axis title")
# Notice that the x-axis starts at 10.  What if we wanted a different set of x-limits? Depends on continuous/discrete
?scale_x_continuous
?scale_x_discrete
g + geom_point() +
scale_x_continuous(limits = c(0, max(mtcars$mpg)))
# One of the most common modifications for y-axis labels is to convert to percents.
# Another common activity is converting data to a percentile via the empirical cumulative distribution function
wt_ecdf <- ecdf(mtcars$wt)
mtcars$wt_pct <- wt_ecdf(mtcars$wt)
# Before
p <- ggplot(mtcars, aes(x = mpg, y = wt_pct))
p + geom_point()
# After
p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1))
# Often times, you'll want to put labels on the data, but we'll need to transform the data into strings with '%'
p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1)) +
geom_text(aes(label = paste0(round(wt_pct,2)*100,"%"), x = mpg, vjust = -1), size = 4)
p <- p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1)) +
labs(x = "MPG", y = "Weight Percentile")
# Black and white theme
p + theme_bw() # this is the best theme, imo
# Can we see all the theme options at once? Yes, with grid.arrange()
a <- p + theme_base() + labs(title = "Base")
b <- p + theme_bw() + labs(title = "Black and White")
c <- p + theme_calc() + labs(title = "Calc")
d <- p + theme_classic() + labs(title = "Classic")
e <- p + theme_clean() + labs(title = "Clean")
f <- p + theme_dark() + labs(title = "Dark")
g <- p + theme_economist() + labs(title = "Economist")
g + geom_point() +
scale_x_continuous(limits = c(0, max(mtcars$mpg)))
# One of the most common modifications for y-axis labels is to convert to percents.
# Another common activity is converting data to a percentile via the empirical cumulative distribution function
wt_ecdf <- ecdf(mtcars$wt)
mtcars$wt_pct <- wt_ecdf(mtcars$wt)
# Before
p <- ggplot(mtcars, aes(x = mpg, y = wt_pct))
p + geom_point()
# After
p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1))
# Often times, you'll want to put labels on the data, but we'll need to transform the data into strings with '%'
p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1)) +
geom_text(aes(label = paste0(round(wt_pct,2)*100,"%"), x = mpg, vjust = -1), size = 4)
p <- p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1)) +
labs(x = "MPG", y = "Weight Percentile")
# Black and white theme
p + theme_bw() # this is the best theme, imo
# Can we see all the theme options at once? Yes, with grid.arrange()
a <- p + theme_base() + labs(title = "Base")
library(scales)
# Black and white theme
p + theme_bw() # this is the best theme, imo
p <- p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1)) +
labs(x = "MPG", y = "Weight Percentile")
# After
p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1))
# Often times, you'll want to put labels on the data, but we'll need to transform the data into strings with '%'
p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1)) +
geom_text(aes(label = paste0(round(wt_pct,2)*100,"%"), x = mpg, vjust = -1), size = 4)
p <- p + geom_point() +
scale_y_continuous(limits = c(0,1),
labels = scales::percent_format(accuracy = 1)) +
labs(x = "MPG", y = "Weight Percentile")
# Black and white theme
p + theme_bw() # this is the best theme, imo
library(tidyverse)
library(scales)
library(gridExtra)
library(ggthemes)
# Can we see all the theme options at once? Yes, with grid.arrange()
a <- p + theme_base() + labs(title = "Base")
b <- p + theme_bw() + labs(title = "Black and White")
c <- p + theme_calc() + labs(title = "Calc")
d <- p + theme_classic() + labs(title = "Classic")
e <- p + theme_clean() + labs(title = "Clean")
f <- p + theme_dark() + labs(title = "Dark")
g <- p + theme_economist() + labs(title = "Economist")
h <- p + theme_economist_white() + labs(title = "Economist White")
i <- p + theme_excel() + labs(title = "Excel")
j <- p + theme_fivethirtyeight() + labs(title = "Five Thirty Eight")
k <- p + theme_gdocs() + labs(title = "Google Docs")
l <- p + theme_minimal() + labs(title = "Minimal")
m <- p + theme_solarized() + labs(title = "Solarized")
n <- p + theme_tufte() + labs(title = "Tufte")
o <- p + theme_wsj() + labs(title = "Wall Street Journal")
q <- p + theme_void() + labs(title = "Void")
grid.arrange(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, q,
ncol = 4,
nrow = 4)
grid.arrange(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, q,
ncol = 4,
nrow = 4)
g <- ggplot(mtcars, aes(x = mpg, y = hp))
g + geom_point() +
facet_grid(~cyl) +
theme_bw()
# What if we want to show some sort of trend?
g + geom_point() +
facet_grid(~cyl) +
theme_bw() +
geom_smooth() # loess method is default
g + geom_point() +
facet_grid(~cyl) +
theme_bw() +
geom_smooth(method = "lm", # linear model
se = FALSE) # remove standard error
g <- ggplot(mtcars, aes(x = mpg, y = hp, color = cyl))
g + geom_point()+
theme_bw()
install.packages("shiny")
install.packages(c("car", "ISLR"))
(Boston)
names(Boston)
### Boston data, housing values in suburbs of Bostons, looks at crime, age, room/dwelling, and other metrics
library(MASS)
library(ISLR)
### fix command
fix(Boston)
names(Boston)
names(Boston)
### runs into error
lm.fit=lm(medv~lstat)
### these lines fix it as it attaches the dataset to the workspace
lm.fit=lm(medv~lstat,data=Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
### completes fit based on economic status of population
lm.fit
summary(lm.fit)
names(lm.fit)
### gives more information than the simple fit such as t stats, p value, etc
### high p-val therefore large correlation between lsat and housing value outcome
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
confint(lm.fit)
### predicts outcome using data for given lstat values
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="prediction")
plot(lstat,medv)
abline(lm.fit)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="red")
plot(lstat,medv,col="red")
plot(lstat,medv,pch=20)
plot(lstat,medv,pch="+")
plot(1:20,1:20,pch=1:20)
par(mfrow=c(2,2))
plot(lstat,medv,pch="+")
plot(1:20,1:20,pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
lm.fit=lm(medv~lstat+age,data=Boston)
summary(lm.fit)
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
summary(lm.fit)
library(car)
summary(lm(medv~lstat*age,data=Boston))
vif(lm.fit)
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
lm.fit1=update(lm.fit, ~.-age)
summary(lm(medv~lstat*age,data=Boston))
# Non-linear Transformations of the Predictors
### non-linear models
lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
plot(lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
summary(lm(medv~log(rm),data=Boston))
fix(Carseats)
names(Carseats)
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
attach(Carseats)
contrasts(ShelveLoc)
LoadLibraries
LoadLibraries()
LoadLibraries=function(){
library(ISLR)
library(MASS)
print("The libraries have been loaded.")
}
LoadLibraries
LoadLibraries()
library(ISLR)
set.seed(1)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
attach(Auto)
### calculates test error
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
set.seed(2)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
glm.fit=glm(mpg~horsepower,data=Auto)
coef(glm.fit)
lm.fit=lm(mpg~horsepower,data=Auto)
coef(lm.fit)
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
cv.error=rep(0,5)
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10)
```
sample(30,4)
```
```
sample(30,4)
choose(30,4)
```
choose(pop_size, sample) command shows number of unique possible samples
``` {r, echo=TRUE}
sample(30,4)
choose(30,4)
```
```
x = sample(30,4)
y = choose(30,4)
```
p <- 0.23
partA <- dbinom(x,n,p)
p <- 0.23
x <- 2
n <- 12
partA <- dbinom(x,n,p)
partB <- 1 - pbinom(p,x,n)
partB <- 1 - pbinom(x,n,p)
x
n
p
### P(X >= 3) = 1 - P(X <= 2)
partB <- 1 - pbinom(x,n,p)
partB
source('~/FHCRC_ML_Project/02_17_2021_gbm_analysis.R', echo=TRUE)
model_ext_list <- list(model_info = list())
for (i in lr_range) {
for (j in id_range) {
m <- gbm(stk_isc~.,
data = train_df,
distribution = "bernoulli",
n.trees = 1000,
shrinkage = i,
interaction.depth = j)
m_rmse <- sqrt(min(m$cv.error))
models_ext_list <- append(models_ext_list, c(m, m_rmse, c(i, j)))
}
}
model_ext_list <- list(model_info = list())
for (i in lr_range) {
for (j in id_range) {
m <- gbm(stk_isc~.,
data = train_df,
distribution = "bernoulli",
n.trees = 1000,
shrinkage = i,
interaction.depth = j)
m_rmse <- sqrt(min(m$cv.error))
models_ext_list <- append(models_ext_list, c(m, m_rmse, c(i, j)))
}
}
source('~/FHCRC_ML_Project/02_17_2021_gbm_analysis.R', echo=TRUE)
for (i in lr_range) {
for (j in id_range) {
m <- gbm(stk_isc~.,
data = train_df,
distribution = "bernoulli",
n.trees = 1000,
shrinkage = i,
interaction.depth = j)
m_rmse <- sqrt(min(m$cv.error))
model_ext_list <- append(model_ext_list, c(m, m_rmse, c(i, j)))
}
}
warnings()
View(model_ext_list)
for (i in lr_range) {
for (j in id_range) {
m <- gbm(stk_isc~.,
data = train_df,
distribution = "bernoulli",
n.trees = 1000,
shrinkage = i,
interaction.depth = j,
cv.folds = 10)
m_rmse <- sqrt(min(m$cv.error))
model_ext_list <- append(model_ext_list, c(m, m_rmse, c(i, j)))
}
}
View(model_ext_list)
View(model_ext_list)
View(model_ext_list)
View(model_ext_list)
model_ext_list
outer_list <- list()
inner_list1 <- list()
inner_list1$mse <- 5
inner_list1$params <- 7
outer_list[[1]] <- inner_list1
